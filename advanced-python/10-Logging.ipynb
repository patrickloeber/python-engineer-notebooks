{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/python-engineer/python-engineer-notebooks/blob/master/advanced-python/10-Logging.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "The logging module in Python is a powerful built-in module so you can quickly add logging to your application.  \n",
    "`import logging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Level\n",
    "There are 5 different log levels indicating the serverity of events. By default, the system logs only events with level *WARNING* and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This is a warning message\n",
      "ERROR:root:This is an error message\n",
      "CRITICAL:root:This is a critical message\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.debug('This is a debug message')\n",
    "logging.info('This is an info message')\n",
    "logging.warning('This is a warning message')\n",
    "logging.error('This is an error message')\n",
    "logging.critical('This is a critical message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "With `basicConfig(**kwargs)` you can customize the root logger. The most common parameters are the *level*, the *format*, and the *filename*. See https://docs.python.org/3/library/logging.html#logging.basicConfig for all possible arguments. See also https://docs.python.org/3/library/logging.html#logrecord-attributes for possible formats and https://docs.python.org/3/library/time.html#time.strftime how to set the time string. Note that this function should only be called once, and typically first thing after importing the module. It has no effect if the root logger already has handlers configured. For example calling `logging.info(...)` before the *basicConfig* will already set a handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n",
    "# Now also debug messages will get logged with a different format.\n",
    "logging.debug('Debug message')\n",
    "\n",
    "# This would log to a file instead of the console.\n",
    "# logging.basicConfig(level=logging.DEBUG, filename='app.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging in modules and logger hierarchy\n",
    "Best practice in your application with multiple modules is to create an internal logger using the `__name__` global variable. This will create a logger with the name of your module and ensures no name collisions. The logging module creates a hierarchy of loggers, starting with the root logger, and adding the new logger to this hierarchy. If you then import your module in another module, log messages can be associated with the correct module through the logger name. Note that changing the basicConfig of the root logger will also affect the log events of the other (lower) loggers in the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper.py\n",
    "# -------------------------------------\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('HELLO')\n",
    "\n",
    "# main.py\n",
    "# -------------------------------------\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n",
    "import helper\n",
    "\n",
    "# --> Output when running main.py\n",
    "# helper - INFO - HELLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation\n",
    "By default, all created loggers will pass the log events to the handlers of higher loggers, in addition to any handlers attached to the created logger. You can deactivate this by setting `propagate = False`. Sometimes when you wonder why you don't see log messages from another module, then this property may be the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper.py\n",
    "# -------------------------------------\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.propagate = False\n",
    "logger.info('HELLO')\n",
    "\n",
    "# main.py\n",
    "# -------------------------------------\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n",
    "import helper\n",
    "\n",
    "# --> No output when running main.py since the helper module logger does not propagate its messages to the root logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogHandlers\n",
    "Handler objects are responsible for dispatching the appropriate log messages to the handler's specific destination. For example you can use different handlers to send log messaged to the standard output stream, to files, via HTTP, or via Email. Typically you configure each handler with a level (`setLevel()`), a formatter (`setFormatter()`), and optionally a filter (`addFilter()`). See https://docs.python.org/3/howto/logging.html#useful-handlers for possible built-in handlers. Of course you can also implement your own handlers by deriving from these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create handlers\n",
    "stream_handler = logging.StreamHandler()\n",
    "file_handler = logging.FileHandler('file.log')\n",
    "\n",
    "# Configure level and formatter and add it to handlers\n",
    "stream_handler.setLevel(logging.WARNING) # warning and above is logged to the stream\n",
    "file_handler.setLevel(logging.ERROR) # error and above is logged to a file\n",
    "\n",
    "stream_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(stream_format)\n",
    "file_handler.setFormatter(file_format)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(stream_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.warning('This is a warning') # logged to the stream\n",
    "logger.error('This is an error') # logged to the stream AND the file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InfoFilter(logging.Filter):\n",
    "    \n",
    "    # overwrite this method. Only log records for which this\n",
    "    # function evaluates to True will pass the filter.\n",
    "    def filter(self, record):\n",
    "        return record.levelno == logging.INFO\n",
    "\n",
    "# Now only INFO level messages will be logged\n",
    "stream_handler.addFilter(InfoFilter())\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other configuration methods\n",
    "We have seen how to configure logging creating loggers, handlers, and formatters explicitely in code. There are two other configration methods:\n",
    "- Creating a logging config file and reading it using the `fileConfig()` function. See example below.\n",
    "- Creating a dictionary of configuration information and passing it to the `dictConfig()` function. See https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig for more information.\n",
    "\n",
    "#### .conf file\n",
    "Create a *.conf* (or sometimes stored as *.ini*) file, define the loggers, handlers, and formatters and provide the names as keys. After their names are defined, they are configured by adding the words *logger*, *handler*, and *formatter* before their names separated by an underscore. Then you can set the properties for each logger, handler, and formatter. In the example below, the root logger and a logger named *simpleExample* will be configured with a StreamHandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging.conf\n",
    "[loggers]\n",
    "keys=root,simpleExample\n",
    "\n",
    "[handlers]\n",
    "keys=consoleHandler\n",
    "\n",
    "[formatters]\n",
    "keys=simpleFormatter\n",
    "\n",
    "[logger_root]\n",
    "level=DEBUG\n",
    "handlers=consoleHandler\n",
    "\n",
    "[logger_simpleExample]\n",
    "level=DEBUG\n",
    "handlers=consoleHandler\n",
    "qualname=simpleExample\n",
    "propagate=0\n",
    "\n",
    "[handler_consoleHandler]\n",
    "class=StreamHandler\n",
    "level=DEBUG\n",
    "formatter=simpleFormatter\n",
    "args=(sys.stdout,)\n",
    "\n",
    "[formatter_simpleFormatter]\n",
    "format=%(asctime)s - %(name)s - %(levelname)s - %(message)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then use the config file in the code\n",
    "import logging\n",
    "import logging.config\n",
    "\n",
    "logging.config.fileConfig('logging.conf')\n",
    "\n",
    "# create logger with the name from the config file. \n",
    "# This logger now has StreamHandler with DEBUG Level and the specified format\n",
    "logger = logging.getLogger('simpleExample')\n",
    "\n",
    "logger.debug('debug message')\n",
    "logger.info('info message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture Stack traces\n",
    "Logging the traceback in your exception logs can be very helpful for troubleshooting issues. You can capture the traceback in *logging.error()* by setting the *exc_info* parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:list index out of range\n",
      "ERROR:root:list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-df97a133cbe6>\", line 5, in <module>\n",
      "    value = a[3]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "try:\n",
    "    a = [1, 2, 3]\n",
    "    value = a[3]\n",
    "except IndexError as e:\n",
    "    logging.error(e)\n",
    "    logging.error(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't capture the correct Exception, you can also use the *traceback.format_exc()* method to log the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    a = [1, 2, 3]\n",
    "    value = a[3]\n",
    "except:\n",
    "    logging.error(\"uncaught exception: %s\", traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotating FileHandler\n",
    "When you have a large application that logs many events to a file, and you only need to keep track of the most recent events, then use a RotatingFileHandler that keeps the files small.\n",
    "When the log reaches a certain number of bytes, it gets \"rolled over\". You can also keep multiple backup log files before overwriting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# roll over after 2KB, and keep backup logs app.log.1, app.log.2 , etc.\n",
    "handler = RotatingFileHandler('app.log', maxBytes=2000, backupCount=5)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "for _ in range(10000):\n",
    "    logger.info('Hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimedRotatingFileHandler\n",
    "If your application will be running for a long time, you can use a TimedRotatingFileHandler. This will create a rotating log based on how much time has passed. Possible time conditions for the *when* parameter are:\n",
    "- second (s)\n",
    "- minute (m)\n",
    "- hour (h)\n",
    "- day (d)\n",
    "- w0-w6 (weekday, 0=Monday)\n",
    "- midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    " \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# This will create a new log file every minute, and 5 backup files with a timestamp before overwriting old logs.\n",
    "handler = TimedRotatingFileHandler('timed_test.log', when='m', interval=1, backupCount=5)\n",
    "logger.addHandler(handler)\n",
    " \n",
    "for i in range(6):\n",
    "    logger.info('Hello, world!')\n",
    "    time.sleep(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging in JSON Format\n",
    "If your application generates many logs from different modules, and especially in a microservice architecture, it can be challenging to locate the important logs for your analysis. Therefore, it is best practice to log your messages in JSON format, and send them to a centralized log management system. Then you can easily search, visualize, and analyze your log records.  \n",
    "I would recommend using this Open Source JSON logger: https://github.com/madzak/python-json-logger  \n",
    "`pip install python-json-logger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "logger.addHandler(logHandler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythonengineer_env]",
   "language": "python",
   "name": "conda-env-pythonengineer_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
